{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "#!pip install opencv-python\n",
    "import os,cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "from keras.layers import Merge\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD,RMSprop,adam\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Reading and Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getcwd()\n",
    "# Define data path\n",
    "data_path = 'C:/Users/albab.ahmad/Desktop/Deep Learning/Project/OCT_Dataset'\n",
    " \n",
    "data_dir_list = os.listdir(data_path)\n",
    "\n",
    "img_rows=128\n",
    "img_cols=128\n",
    "num_channel=1\n",
    "num_epoch=2\n",
    "no_images=0\n",
    "\n",
    "for dataset in data_dir_list:\n",
    "    img_list = os.listdir(data_path + '/' + dataset)\n",
    "    no_images = no_images+len(img_list)\n",
    "\n",
    "# Define the number of classes\n",
    "labels = np.ones((no_images,),dtype='int64')\n",
    "num_classes = 3\n",
    "label_index=0\n",
    "img_data_list=[]\n",
    "img=0\n",
    "\n",
    "for dataset in data_dir_list:\n",
    "    img_list=os.listdir(data_path+'/'+ dataset)\n",
    "    print ('Loaded the images of dataset-'+'{}\\n'.format(dataset))\n",
    "    for img in img_list:\n",
    "        input_img=cv2.imread(data_path + '/'+ dataset + '/'+ img)\n",
    "        input_img=cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n",
    "        input_img_resize=cv2.resize(input_img,(128,128))\n",
    "        img_data_list.append(input_img_resize)\n",
    "        if dataset[0]==  'A':\n",
    "            labels[label_index]=  0\n",
    "            #print(dataset[0])\n",
    "        if dataset[0] == 'D':\n",
    "            labels[label_index] = 1\n",
    "            #print(dataset[0])\n",
    "        if dataset[0] == 'N':\n",
    "            labels[label_index] = 2\n",
    "        label_index = label_index+1\n",
    "            #print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img_data = np.array(img_data_list)\n",
    "img_data = img_data.astype('float32')\n",
    "img_data /= 255\n",
    "print (img_data.shape)\n",
    "\n",
    "\n",
    "# Using 'th' for the image_dim_ordering we get accuracy >=0.99 . \n",
    "# Using 'tf' for the dim order I get accuracy >= 0.9 but on more epochs\n",
    "if num_channel==1:\n",
    "    if K.image_dim_ordering()=='th':\n",
    "        img_data= np.expand_dims(img_data, axis=1)\n",
    "        print (img_data.shape)\n",
    "    else:\n",
    "        img_data= np.expand_dims(img_data, axis=4)\n",
    "        print (img_data.shape)\n",
    "\n",
    "else:\n",
    "    if K.image_dim_ordering()=='th':\n",
    "        img_data=np.rollaxis(img_data,3,1)\n",
    "        print (img_data.shape)\n",
    "\n",
    "\n",
    "        labels[0:722] = 0\n",
    "        labels[723:1823] = 1\n",
    "        labels[1824:3231] = 2\n",
    "        \n",
    "        \n",
    "        X_train.shape\n",
    "        \n",
    "\n",
    "        \n",
    "USE_SKLEARN_PREPROCESSING=False\n",
    "\n",
    "if USE_SKLEARN_PREPROCESSING:\n",
    "    # using sklearn for preprocessing\n",
    "    from sklearn import preprocessing\n",
    "\n",
    "    def image_to_feature_vector(image, size=(128, 128)):\n",
    "        # resize the image to a fixed size, then flatten the image into\n",
    "        # a list of raw pixel intensities\n",
    "        return cv2.resize(image, size).flatten()\n",
    "\n",
    "    img_data_list=[]\n",
    "    for dataset in data_dir_list:\n",
    "        img_list=os.listdir(data_path+'/'+ dataset)\n",
    "        print ('Loaded the images of dataset-'+'{}\\n'.format(dataset))\n",
    "        for img in img_list:\n",
    "            input_img=cv2.imread(data_path + '/'+ dataset + '/'+ img )\n",
    "            input_img=cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n",
    "            input_img_flatten=image_to_feature_vector(input_img,(128,128))\n",
    "            img_data_list.append(input_img_flatten)\n",
    "\n",
    "    img_data = np.array(img_data_list)\n",
    "    img_data = img_data.astype('float32')\n",
    "    print (img_data.shape)\n",
    "    img_data_scaled = preprocessing.scale(img_data)\n",
    "    print (img_data_scaled.shape)\n",
    "\n",
    "    if K.image_dim_ordering()=='th':\n",
    "        img_data_scaled=img_data_scaled.reshape(img_data.shape[0],num_channel,img_rows,img_cols)\n",
    "        print (img_data_scaled.shape)\n",
    "\n",
    "    else:\n",
    "        img_data_scaled=img_data_scaled.reshape(img_data.shape[0],img_rows,img_cols,num_channel)\n",
    "        print (img_data_scaled.shape)\n",
    "\n",
    "\n",
    "    if K.image_dim_ordering()=='th':\n",
    "        img_data_scaled=img_data_scaled.reshape(img_data.shape[0],num_channel,img_rows,img_cols)\n",
    "        print (img_data_scaled.shape)\n",
    "\n",
    "    else:\n",
    "        img_data_scaled=img_data_scaled.reshape(img_data.shape[0],img_rows,img_cols,num_channel)\n",
    "        print (img_data_scaled.shape)\n",
    "\n",
    "if USE_SKLEARN_PREPROCESSING:\n",
    "    img_data=img_data_scaled\n",
    "    \n",
    "    #%%\n",
    "labels[0:1000]\n",
    "#%%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning Labels\n",
    "\n",
    "# Define the number of classes\n",
    "num_classes = 3\n",
    "\n",
    "names = ['AMD','DME','NORMAL']\n",
    "\n",
    "# convert class labels to on-hot encoding\n",
    "Y = np_utils.to_categorical(labels, num_classes)\n",
    "\n",
    "#Shuffle the dataset with random state=2\n",
    "x,y = None\n",
    "# Split the dataset with 20% testing data\n",
    "X_train, X_test, y_train, y_test = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model \n",
    "# Feel free to use CNNs/Dense Networks\n",
    "\n",
    "model = Sequential()\n",
    "model.add(None)\n",
    "\n",
    "\n",
    "# Viewing model_configuration\n",
    "model.summary()\n",
    "#model.get_config()\n",
    "#model.layers[0].get_config()\n",
    "#model.layers[0].input_shape\n",
    "#model.layers[0].output_shape\n",
    "#model.layers[0].output\n",
    "#model.layers[0].get_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and fit wit appropiate batch size, epochs, verbose = 1 and validation set\n",
    "hist = None\n",
    "\n",
    "# model saving \n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# Save the model in hdf5 file\n",
    "None\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model w.r.t Test Loss and Test Accuracy\n",
    "score = None\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict model on Test Data\n",
    "\n",
    "Y_pred = None\n",
    "print(Y_pred)\n",
    "\n",
    "# Printing the confusion matrix\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import itertools\n",
    "\n",
    "# Print the classes of the Prediction\n",
    "y_pred = None\n",
    "print(y_pred)\n",
    "\n",
    "target_names = ['AMD', 'DME', 'Normal']\n",
    "                                        \n",
    "# Plotting the confusion matrix\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "#Computation  confusion matrix\n",
    "cnf_matrix = (confusion_matrix(np.argmax(y_test,axis=1), y_pred))\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Plotting non-normalized confusion matrix\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names,\n",
    "                      title='Confusion matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
